{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4380f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_layers, neurons_per_layer, learning_rate=0.01):\n",
    "        self.num_layers = num_layers\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.learning_rate = learning_rate  # Initialize learning rate\n",
    "        self.init_weights_and_biases()\n",
    "        \n",
    "    def init_weights_and_biases(self):\n",
    "        for i in range(1, len(self.neurons_per_layer)):\n",
    "            weight = np.random.randn(self.neurons_per_layer[i], self.neurons_per_layer[i-1]) * 0.01\n",
    "            bias = np.zeros((self.neurons_per_layer[i], 1))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def compute_loss(self, y_true, scores):\n",
    "        num_samples = scores.shape[1]\n",
    "        correct_class_scores = scores[y_true, np.arange(num_samples)]\n",
    "        margins = np.maximum(0, scores - correct_class_scores + 1)  # Delta = 1\n",
    "        margins[y_true, np.arange(num_samples)] = 0\n",
    "        loss = np.sum(margins) / num_samples\n",
    "        return loss\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        cache = {\"A0\": X}\n",
    "        A = X\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases), 1):\n",
    "            Z = np.dot(w, A) + b\n",
    "            A = self.relu(Z)\n",
    "            cache[\"Z\" + str(i)] = Z\n",
    "            cache[\"A\" + str(i)] = A\n",
    "        return A, cache\n",
    "    \n",
    "    def backward_propagation(self, y_true, cache):\n",
    "        num_samples = cache[\"A0\"].shape[1]\n",
    "        scores = cache[\"A\" + str(self.num_layers - 1)]\n",
    "        grad_scores = np.zeros_like(scores)\n",
    "\n",
    "        # Correct class scores calculation fix\n",
    "        for i in range(num_samples):\n",
    "            correct_class_score = scores[y_true[i], i]\n",
    "            for j in range(scores.shape[0]):\n",
    "                if j == y_true[i]:\n",
    "                    continue\n",
    "                margin = scores[j, i] - correct_class_score + 1  # Delta = 1\n",
    "                if margin > 0:\n",
    "                    grad_scores[j, i] += 1\n",
    "                    grad_scores[y_true[i], i] -= 1\n",
    "\n",
    "        # Backpropagate the gradient to the weights and biases\n",
    "        grad_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        grad_A = grad_scores\n",
    "        for l in reversed(range(self.num_layers - 1)):\n",
    "            grad_Z = grad_A * self.relu_derivative(cache[\"Z\" + str(l + 1)])\n",
    "            grad_A = np.dot(self.weights[l].T, grad_Z)\n",
    "            grad_weights[l] = np.dot(grad_Z, cache[\"A\" + str(l)].T) / num_samples\n",
    "            grad_biases[l] = np.sum(grad_Z, axis=1, keepdims=True) / num_samples\n",
    "\n",
    "            # Update weights and biases based on gradient\n",
    "            self.weights[l] -= self.learning_rate * grad_weights[l]\n",
    "            self.biases[l] -= self.learning_rate * grad_biases[l]\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores, _ = self.forward_propagation(X)\n",
    "        predictions = np.argmax(scores, axis=0)\n",
    "        return predictions\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            scores, cache = self.forward_propagation(X)\n",
    "            loss = self.compute_loss(y, scores)\n",
    "            self.backward_propagation(y, cache)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ede2147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.999759284784261\n",
      "Epoch 100, Loss: 0.9527250348308439\n",
      "Epoch 200, Loss: 0.7189462420704354\n",
      "Epoch 300, Loss: 0.7052370547438914\n",
      "Epoch 400, Loss: 0.7014653376488726\n",
      "Epoch 500, Loss: 0.7000808716581571\n",
      "Epoch 600, Loss: 0.6985573354839751\n",
      "Epoch 700, Loss: 0.6968792731300595\n",
      "Epoch 800, Loss: 0.6950774178858458\n",
      "Epoch 900, Loss: 0.6930381790604491\n",
      "Epoch 1000, Loss: 0.6912879359349624\n",
      "Epoch 1100, Loss: 0.6901691471110138\n",
      "Epoch 1200, Loss: 0.6889013225228344\n",
      "Epoch 1300, Loss: 0.6872990696392987\n",
      "Epoch 1400, Loss: 0.6859247639732898\n",
      "Epoch 1500, Loss: 0.6848874867634323\n",
      "Epoch 1600, Loss: 0.6837709108714443\n",
      "Epoch 1700, Loss: 0.6831448318288851\n",
      "Epoch 1800, Loss: 0.682108772895567\n",
      "Epoch 1900, Loss: 0.6818750358176879\n",
      "Test set accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 3: Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Assuming you have the NeuralNetwork class defined and ready\n",
    "# Step 5: Initialize the neural network\n",
    "# Note: Adjust the number of neurons in the layers according to your needs\n",
    "nn = NeuralNetwork(num_layers=3, neurons_per_layer=[4, 10, 3], learning_rate = 0.1)\n",
    "\n",
    "# Step 6: Train the neural network\n",
    "nn.fit(X_train_scaled.T, y_train, epochs=2000)\n",
    "\n",
    "\n",
    "# Step 7: Evaluate the performance\n",
    "predictions = nn.predict(X_test_scaled.T)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test set accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e7ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
